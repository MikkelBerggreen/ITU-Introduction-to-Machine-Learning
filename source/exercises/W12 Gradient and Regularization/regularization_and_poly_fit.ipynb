{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "8d88a0a3",
            "metadata": {},
            "source": [
                "# Regularization of Linear Models with SKLearn\n",
                "Linear models are usually a good starting point for training a model. However, a lot of datasets do not exhibit linear relationships between the independent and the dependent variables. As a result, it is frequently necessary to create a polynomial model. However, these models are usually prone to overfitting. One method of reducing overfitting in polynomial models is through the use of regularization.\n",
                "Let\u2019s import the necessary libraries and load up our training dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "3031c478",
            "metadata": {},
            "outputs": [],
            "source": [
                "#imports\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import math\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "sns.set()\n",
                "%matplotlib inline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb5e9655",
            "metadata": {},
            "source": [
                "Let\u2019s split our data into a training set and a validation set as you did before. You will hold out 30% of the data for validation. You will use a random state to make our experiment reproducible.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "3fbc6d20",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
                "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
                "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
                "y = raw_df.values[1::2, 2]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f519d90b",
            "metadata": {},
            "source": [
                "Let\u2019s establish a baseline by training a linear regression model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "14aecf7a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training score: 0.7434997532004697\n",
                        "Test score: 0.7112260057484974\n",
                        "RMSE_train: 4.748208239685937\n",
                        "RMSE_test: 4.638689926172788\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "lr_model = LinearRegression()\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "print('Training score: {}'.format(lr_model.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(lr_model.score(X_test, y_test)))\n",
                "\n",
                "y_pred_train = lr_model.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = lr_model.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "\n",
                "print('RMSE_test: {}'.format(rmse_test))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d9bcb02a",
            "metadata": {},
            "source": [
                "The model above should give us a training accuracy and a test accuracy of about 72%. We should also get an RMSE of about 4.5. The next models we train should outperform this model with higher accuracy scores and a lower RMSE.\n",
                "We need to engineer new features. Specifically, we need to create polynomial features by taking our individual features and raising them to a chosen power. Thankfully, scikit-learn has an implementation for this and we don\u2019t need to do it manually.\n",
                "Something else we would like to do is standardize our data. This scales our data down to a range between 0 and 1. This serves the purpose of letting us work with reasonable numbers when we raise to a power.\n",
                "Finally, because we need to carry out the same operations on our training, validation, and test sets, we will introduce a pipeline. This will let us pipe our process so the same steps get carried out repeatedly.\n",
                "To summarize, we will scale our data, then create polynomial features, and then train a linear regression model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "ea962485",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 2.162970056950185\n",
                        "RMSE_test: 5.0811876783255245\n",
                        "\n",
                        "Training score: 0.9467733311147442\n",
                        "Test score: 0.6535042863861226\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', LinearRegression())\n",
                "]\n",
                "\n",
                "pipeline = Pipeline(steps)\n",
                "\n",
                "pipeline.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = pipeline.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = pipeline.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}\\n'.format(rmse_test))\n",
                "\n",
                "print('Training score: {}'.format(pipeline.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(pipeline.score(X_test, y_test)))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "540614ce",
            "metadata": {},
            "source": [
                "After running our code, we will get a training accuracy of about 94.75%, and a test accuracy of 46.76%. This is a sign of overfitting. It\u2019s normally not a desirable feature, but that is exactly what we were hoping for.\n",
                "We will now apply regularization to our new data.\n",
                "## l2 Regularization or Ridge Regression\n",
                "To understand Ridge Regression, we need to remind ourselves of what happens during gradient descent, when our model coefficients are trained. During training, our initial weights are updated according to a gradient update rule using a learning rate and a gradient. Ridge regression adds a penalty to the update, and as a result shrinks the size of our weights. This is implemented in scikit-learn as a class called Ridge.\n",
                "We will create a new pipeline, this time using Ridge. We will specify our regularization strength by passing in a parameter, alpha. This can be really small, like 0.001, or as large as you would want it to be. The larger the value of alpha, the less variance your model will exhibit.\n",
                "\n",
                "${\\begin{align*}\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y_{n}-\\theta^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right\\}^{2}+\\frac{\\lambda}{2}\\|\\theta\\|_2^2\n",
                "\\end{align*}}$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "5778e738",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 2.441071076959751\n",
                        "RMSE_test: 3.823376123713985\n",
                        "Training Score: 0.9322063334864212\n",
                        "Test Score: 0.8038169683868278\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', Ridge(alpha=10, fit_intercept=True))\n",
                "]\n",
                "\n",
                "ridge_pipe = Pipeline(steps)\n",
                "ridge_pipe.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = ridge_pipe.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = ridge_pipe.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}'.format(rmse_test))\n",
                "\n",
                "print('Training Score: {}'.format(ridge_pipe.score(X_train, y_train)))\n",
                "print('Test Score: {}'.format(ridge_pipe.score(X_test, y_test)))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2e643082",
            "metadata": {},
            "source": [
                "By executing the code, we should have a training accuracy of about 91.8%, and a test accuracy of about 82.87%. That is an improvement on our baseline linear regression model.\n",
                "Let\u2019s try something else.\n",
                "## l1 Regularization or Lasso Regression\n",
                "By creating a polynomial model, we created additional features. The question we need to ask ourselves is which of our features are relevant to our model, and which are not.\n",
                "l1 regularization tries to answer this question by driving the values of certain coefficients down to 0. This eliminates the least important features in our model. We will create a pipeline similar to the one above, but using Lasso. You can play around with the value of alpha.\n",
                "\n",
                "${\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y_{n}-\\theta^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right\\}^{2}+\\frac{\\lambda}{2}\\|\\theta\\|_1 }$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ce3f0b2e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 3.538738418298479\n",
                        "RMSE_test: 3.970165571442558\n",
                        "Training score: 0.8575294192309941\n",
                        "Test score: 0.7884638325042947\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', Lasso(alpha=0.3, fit_intercept=True))\n",
                "]\n",
                "\n",
                "lasso_pipe = Pipeline(steps)\n",
                "\n",
                "lasso_pipe.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = lasso_pipe.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = lasso_pipe.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}'.format(rmse_test))\n",
                "\n",
                "print('Training score: {}'.format(lasso_pipe.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(lasso_pipe.score(X_test, y_test)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "89a7a197",
            "metadata": {},
            "source": [
                "# task :\n",
                "In Exercise 9 task 3, you have found the otipmal complexity for the polynomial model. In this task you will use that polynomial Regression and will apply Ridge and and Lasso Regression to it:\n",
                "1. Use optimal degree you have found for the polynomial and calculate RMSE_train and RMSE_test for it.\n",
                "2. Use the polynomial degree=10 and apply Ridge Regression  find the optimal lambda (around 0.001) and calculate RMSE_train and RMSE_test\n",
                "3. Use the polynomial degree=10 and apply Lasso Regression and find the optimal lambda (around 0.001) and calculate RMSE_train and RMSE_test\n",
                "4. In the results you can see RMSE_test using regularizaition is lower than polynomial with optimal complexity. How you justify these results?  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "32402b0e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train/Test Size :  (18,) (12,) (18,) (12,)\n",
                        "RMSE_train: 0.056532247455264445\n",
                        "RMSE_test: 0.30856057840599427\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "#use this code for your solutions \n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#cosin function\n",
                "def true_fun(X):\n",
                "    return np.cos(1.5 * np.pi * X)\n",
                "\n",
                "np.random.seed(0)\n",
                "\n",
                "n_samples = 30\n",
                "\n",
                "X = np.sort(np.random.rand(n_samples))\n",
                "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
                "\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.60, test_size=0.40, random_state=1)\n",
                "print('Train/Test Size : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
                "degree=10\n",
                "polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
                "linear_regression = LinearRegression()\n",
                "pipeline = Pipeline(\n",
                "    [\n",
                "        (\"polynomial_features\", polynomial_features),\n",
                "        (\"linear_regression\", linear_regression),\n",
                "    ]\n",
                ")\n",
                "pipeline.fit(X_train[:, np.newaxis], Y_train)\n",
                "\n",
                "y_pred_train = pipeline.predict(X_train[:, np.newaxis])\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = pipeline.predict(X_test[:, np.newaxis])\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}\\n'.format(rmse_test))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1d8618c2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train/Test Size :  (18,) (12,) (18,) (12,)\n",
                        "RMSE_train: 0.1079839948207335\n",
                        "RMSE_test: 0.12863714132664852\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your  code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 155,
            "id": "65230244",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 0.1248997128894005\n",
                        "RMSE_test: 0.12911924203886604\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your  code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "9c958a34",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 0.10041317401921453\n",
                        "RMSE_test: 0.11953621236626978\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "12cda8d4",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}